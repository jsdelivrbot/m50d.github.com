---
published: false
---

A pure function looks the same in almost any programming language. Where today's programming languages differ is in their handling of *effects*: the parts of the program that consist of something other than just substituting a function call with its return value.

[There is an equivalence](http://conal.net/blog/posts/the-c-language-is-purely-functional) between these things: any effect can be captured in a value that "lazily" represents performing that effect, and any manipulation of values could be modelled as an effect. [Assembly language](http://wall.org/~lewis/2013/10/15/asm-monad.html) (or rather machine code) is, paradoxically, the most extreme example of both: a machine-code program consists entirely of effects, because there's not even anywhere to put a value. But equivalently, a machine-code program is a value in the most obvious possible way: it's nothing but a sequence of bytes. Modern languagues make more finer-grained distinctions; in Haskell-style functional languages it's common to have a value (e.g. `GenerateRandomNumber`) that's "interpreted" first in some specific context (e.g. `Rng[Int]`), then in a more general context (e.g. `IO[Int]`), with several layers before we finally reach the stage of a general effect.

To talk of "effects" is inherently to adopt a certain perspective on these things: that they do have something in common and can be treated similarly; in a sense I'm already assuming the Haskell worldview, and the biases that come with it. But hopefully this is enlightening for programmers of other schools, and will maybe shed some new light on certain language design decisions. Unsurprisingly, different programming languages take different approaches to managing effects. What's more interesting is when the same language adopts quite different attitudes to two effects that seem to me quite similar.

Mostly I’ll use Java, Python and Scala as examples, because they’re the languages I’m most familiar with. When I’m aware of a language with a distinctive position on some area, I’ll mention it. Apologies if I misunderstand a feature of a less familiar language.

Compiling this list of examples, a few axes emerged:

 * How an effect appears locally in the source code. Some effects are *magic* (that is, completely invisible) - the language simply manages that effect for you (so it will almost always also be implicit and safe). Some are *concise* and some are *verbose*.
 * How an effect appears more globally, from the function that calls the function in which the effect occurs. Some are *implicit* - a call to a function that performs that effect looks like any other function call. Some are *[special-cased](http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/)* - functions with this effect must be handled differently from other functions, perhaps with language support or perhaps not. And some are *generic* - the effect is visible in the function call, but "effect-ness" can be abstracted over in the lanugage.
 * Whether the use of the effect is *safe* - the language enforces that the effect is handled correctly - or *unsafe* - it's up to the programmer to do that. I also found a distinct class of *semi-safe* effect handling, where there are constructs and simple rules that can be followed to ensure the effect is handled safely, but the language also permits invoking the effect unsafely (perhaps e.g. in legacy libraries).
 
#The examples
##Memory management
Java, Python, Scala: magic, implicit, safe. Most languages these days have automatic garbage collection
C: verbose, implicit, unsafe. C is the canonical example of a language with manual memory management: the programmer explicitly calls `malloc` and `free`, and is responsible for ensuring they're called correctly. Allocation and ownership are implicit beyond function scope: the only way to know whether a function will allocate memory for new data structures, or free that of existing ones, is with out-of-band information like comments.
C++: semi-concise, semi-generic, semi-safe. C++ can often feel like several languages at once (and so some now-common constructs are more verbose than they should be), but provided you use the right constructs (smart pointers, stack allocation with RAII) and follow a few rules then memory safety is enforced. How memory management appears globally is an interesting mix: memory management information is sometimes visible in a function definition (smart pointers, references) but not always. Abstracting memory management generically is interesting: in theory RAII means memory and non-memory resources can be managed in the same way (so e.g. templates can be polymorphic in what kind of resource they’re using), and templates can handle smart pointers or in theory even abstract over reference-ness. But I've seen limited use of these techniques in practice, in part because of the complexity of the template system.
Rust: concise, ???, safe. Rust's big selling point is that memory is manually managed, but safety is enforced (via the borrow checker), with the lifecycle of any function parameter visible in the function signature. I’m not clear on exactly how “lifecycle-polymorphic” functions can be - Rust doesn't have the abstractions you need to treat effects completely generically (i.e. higher-kinded types), but some level of abstraction over borrowed vs. owned vs. shared resources might be possible.

Much has been said about manual memory management; irrelevant to most languages, it's still a hard requirement for a small number of use cases.
##Non-memory resources
Java <=6, Python <=2.4 using manual resources: verbose, implicit, unsafe. The programmer must explicitly free resources, and it usually requires a  `try`/`finally` block to do correctly.
Python <= 2.4 using destructors: magic, implicit, safe-ish. Python uses reference counting so has deterministic(ish) destruction, so it's possible to rely on object destructors to free resources (but at the risk of turning relatively benign memory leaks into more perilous resource leaks). I've even seen code that does this in early Java, but garbage collection makes it very unsafe: it's very possible to e.g. hit the OS file handle limit because your program never runs out of memory so never closes its files.
Python 2.5+ using `with` statements: concise, ???, semi-safe. Use a specific statement to give a resource block scope, safely, but relies on the programmer using that statement. Resources can't cleanly be passed across function boundaries - have to belong to a particular block.
Java 7+ using try-with-resources: concise, ???, safe. As Python, but the compiler will warn if the programmer forgets to use the statement with an `AutoCloseable` object.
Scala using scala-arm: concise, generic, semi-safe. The `Resource[A]` abstraction makes it possible to pass a resource safely across function boundaries, and since it's an ordinary parameterized type we can abstract over it like any other effect. But as in the Python case, the user has to explicitly call `managed` on their resources.
C++ using RAII: Concise, implicit, safe. In C++ resources are usually managed via object construction and destruction; since object lifecycles are already managed carefully for the sake of memory management, we get management of other resources "for free".
Go with “dispose”: concise, ???, ??? - I’ve heard extreme claims about how nice this is. Must investigate further.
##Error handling
Python, Java using unchecked exceptions: magic, implicit, unsafe. Any function might throw, and exceptions might not be caught.
Manual, cumbersome: C. Programmer must explicitly handle errors, with little language-level support.
Manual, moderate-cumbersome: Go. As C but with some checking via the “res, err = …” syntax.
Manual, moderate: Java using checked exceptions. Correctness guaranteed, but requires a dedicated, non-polymorphic language construct.
Manual, moderate-elegant: Rust. Uses ordinary values and provides some degree of abstraction, but not fully polymorphic as the language lacks higher-kinded types.
Scala using \/ or Either. Uses ordinary values and for/yield, interoperable with generics. Mostly checked, but functions can also throw exceptions.
##File I/O
Magic: Java, Python, mainstream Scala. Any function might read or write files.
Manual, moderate-elegant: Scala using IO. Ordinary values and for/yield, interoperable with generics. Only checked when using functions that use it.
Manual, elegant: Haskell. Uses ordinary values, uses general constructs (Monad) that can be abstracted over, checked.
##Database Access
Magic: Traditional Java, Python, mainstream Scala. Any function might access a database. This classification doesn't really distinguish between the actual API (the JDBC API is far more cumbersome than e.g. SQLAlchemy), we're purely talking about effect-tracking here.
Manual, moderate: JPA / Hibernate. Uses annotations, which are concise but hard to abstract over and not refactor-safe. JPA relies on programmer to get transaction boundaries correct. Can be difficult in the presence of async or (to a lesser extent) exceptions.
Manual, elegant: Doobie - ordinary values, general constructs (Monad), inherently safe. (But I actually use Hibernate, as I find the conciseness of an ORM API is worth the safety cost).
##Async
##Audit logging
##Custom constructs
It’s inherently impossible for these to be magic.
Manual, cumbersome: most languages
##Redefining language syntax
#Thoughts
Actually two classifications: safety and consistency.

In the light of this, otherwise paradoxical decisions start to make sense. Java unchecked exceptions. (What does quasar do about exceptions that were thrown on a different thread?)

The biggest divide between contemporary programming languages is what’s allowed to happen "by magic". Magic is the programmer’s greatest foe, as it makes reasoning difficult or impossible. At the same time, some things have to be magic - reasoning about assembly code is just as hard as reasoning about poorly-factored metaclass-heavy Python.

As programming techniques improve, it becomes practical to explicitly manage things that previously had to be left to magic. At the same time, as language runtimes improve, it becomes more acceptable to leave things to magic.